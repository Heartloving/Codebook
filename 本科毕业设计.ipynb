{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import Func\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intro-mnist  莫烦教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 莫烦教程\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "\n",
    "def add_layer(inputs,in_size,out_size,activation_function = None):\n",
    "    Weights = tf.Variable(tf.random_normal([in_size,out_size]),name = 'W')\n",
    "    biases = tf.Variable(tf.zeros([1,out_size])+0.1,name = 'b')\n",
    "    Wx_puls_b = tf.add(tf.matmul(inputs,Weights),biases)\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_puls_b \n",
    "    else:\n",
    "        outputs = activation_function(Wx_puls_b,)\n",
    "    return outputs\n",
    "\n",
    "def compute_accuracy(v_xs,v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction,feed_dict={x:v_xs})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1),tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    result = sess.run(accuracy,feed_dict={x:v_xs,y_:v_ys})\n",
    "    return result\n",
    "\n",
    "#define placeholder for inputs to netword\n",
    "x = tf.placeholder(\"float\", [None, 784])  #28*28\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "#add ouyput layer\n",
    "prediction = add_layer(x,784,10,activation_function = tf.nn.softmax)\n",
    "\n",
    "#the error between prediction and real data 计算交叉熵:\n",
    "#cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(prediction),\n",
    "                                              reduction_indices=[1]))#loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1001):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i%50 == 0:  \n",
    "#         correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#         print (\"step %d, training accuracy %g\"%(i, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))  \n",
    "          print(i,compute_accuracy(mnist.test.images,mnist.test.labels))\n",
    "# saver = tf.train.Saver()   \n",
    "# saver.save(sess, \"model_data_intro/model.ckpt\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intro-mnist  官方教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 官方教程\n",
    "\n",
    "time_start=time.time()\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "\n",
    "#define placeholder for inputs to netword\n",
    "x = tf.placeholder(\"float\", [None, 784])  #28*28\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]),dtype = tf.float32,name = 'weights')\n",
    "b = tf.Variable(tf.zeros([10]),dtype = tf.float32,name = 'biases')\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1001):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if i%100 == 0:  \n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"step %d, training accuracy %g\"%(i, sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})))  \n",
    "saver = tf.train.Saver()   \n",
    "saver.save(sess, \"./model/intro/model.ckpt\")  \n",
    "\n",
    "time_end=time.time();#time.time()为1970.1.1到当前时间的毫秒数  \n",
    "print ((time_end-time_start)/60,\"mins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# expert-mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "\n",
    "time_start=time.time()\n",
    "sess = tf.InteractiveSession()  \n",
    "  \n",
    "#定义一些函数：分配系数函数、分配偏置函数、卷积函数、pooling函数  \n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#均值0标准方差0.1，剔除2倍标准方差之外的随机数据  \n",
    "    return tf.Variable(initial)  \n",
    "    \n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)#统一值0.1  \n",
    "    return tf.Variable(initial)  \n",
    "  \n",
    "def conv2d(x, W):  \n",
    "  #待操作的数据x，模板W，tensor不同维度上的步长，强制与原tensor等大  \n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "  \n",
    "def max_pool_2x2(x):  \n",
    "    #平面数据的pool模板2*2，平面数据滑动步长2*2（非重叠的pool）  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')  \n",
    "  \n",
    "  \n",
    " #x是输入的图像，y_是对应的标签  \n",
    "x = tf.placeholder(tf.float32, [None, 784])  \n",
    "y_ = tf.placeholder(tf.float32, [None, 10])  \n",
    "  \n",
    "#第1层卷积层，Receptive Field 5＊5，单个batch生成32通道数据  \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "  \n",
    "#把图像向量还原成28＊28的图像  \n",
    "x_image = tf.reshape(x, [-1,28,28,1])  \n",
    " \n",
    "#第1个卷积层，使用了ReLU激活函数  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "  \n",
    "#第2层卷积层，Receptive Field 5＊5，单个batch 32通道生成64通道数据  \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "  \n",
    "#第2个卷积层  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "  \n",
    "#全链接层系数  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "  \n",
    "#全链接层：把64通道数据展开方便全链接  \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "  \n",
    "#全链层神经元使用dropout防止过拟合  \n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "  \n",
    "#softmax层系数  \n",
    "W_fc2 = weight_variable([1024, 10])  \n",
    "b_fc2 = bias_variable([10])  \n",
    "  \n",
    "#softmax层  \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "  \n",
    "#交叉熵和训练构型：AdamOptimizer适合这种求和的误差项  \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))  \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  \n",
    "  \n",
    "#验证步骤的构型  \n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  \n",
    "  \n",
    "#初始化  \n",
    "sess.run(tf.initialize_all_variables())  \n",
    "  \n",
    "#开始训练  \n",
    "for i in range(1000):  \n",
    "    batch = mnist.train.next_batch(50)  \n",
    "    if i%100 == 0:  \n",
    "        #验证的时候dropout=1.0，训练时=0.5  \n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})  \n",
    "        print (\"step %d, training accuracy %g\"%(i, train_accuracy))  \n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})  \n",
    "  \n",
    "#验证最终的准确率  \n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}) )  \n",
    "\n",
    "saver = tf.train.Saver()   \n",
    "saver.save(sess, \"./model/expert/model.ckpt\")  \n",
    "\n",
    "time_end=time.time();#time.time()为1970.1.1到当前时间的毫秒数  \n",
    "print ((time_end-time_start)/60,\"mins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 算式图片切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#将算式中的多个字符分别切割保存,保存路径为 file_dir+\"/imgi.png\"\n",
    "def CutImage(img,file_dir):\n",
    "    img = np.asarray(img)\n",
    "    L=0\n",
    "    R=0\n",
    "    lock = 0\n",
    "    cnt=0\n",
    "     #寻找左右边界\n",
    "    for i in range(len(img[0])):\n",
    "        if Func.Judgecolzero(img,i)==1 and lock==0:\n",
    "            L=i\n",
    "            lock=1\n",
    "        if Func.Judgecolzero(img,i)==0 and lock==1:\n",
    "            R=i\n",
    "            lock=0\n",
    "            img1 = im.crop((L-15,0,R+15,len(img)-1))\n",
    "            img2 = Func.Thumbnail(img1.crop(Func.JudgeEdge(img1)),28)\n",
    "            img2.save(file_dir+\"img\"+str(cnt)+\".png\")\n",
    "            L=0\n",
    "            R=0\n",
    "            cnt=cnt+1\n",
    "    return cnt\n",
    "        \n",
    "file_name='images_data/s.png'#导入自己的图片地址\n",
    "\n",
    "im = Image.open(file_name).convert('1')\n",
    "\n",
    "cnt = CutImage(im,\"images_data/img/\")#将算式中的多个字符分别切割保存,保存路径为 file_dir+\"/imgi.png\"\n",
    "print(\"ok\",cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 成绩表格图片切割校正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Adjust_img(file_name):\n",
    "    img = cv2.imread(file_name)\n",
    "    result = img.copy()\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY)\n",
    "    ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY_INV) #反色读入\n",
    "\n",
    "    #binary, contours, hierarchy = cv2.findContours(eroded,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    binary, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "    temp = result[y:(y + h), x:(x + w)]\n",
    "    rect = cv2.minAreaRect(contours[0])\n",
    "    height,width = temp.shape[:2]\n",
    "\n",
    "    up = int((height - rect[1][0])/2)\n",
    "    down = int(height - up)\n",
    "    left = int((width - rect[1][1])/2)\n",
    "    right =int(width - left)\n",
    "    #第一个参数旋转中心，第二个参数旋转角度，第三个参数：缩放比例\n",
    "    M = cv2.getRotationMatrix2D((width/2,height/2),(rect[2]+90),1)\n",
    "    res = cv2.warpAffine(temp,M,(width,height))\n",
    "    temp1 = res[up:down,left:right]\n",
    "    gray1 = cv2.cvtColor(temp1,cv2.COLOR_BGR2GRAY)\n",
    "    ret,thresh1 = cv2.threshold(gray1,190,255,cv2.THRESH_BINARY)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    eroded = cv2.erode(thresh1, kernel)\n",
    "    \n",
    "    file = \"images_data/img/adjust.png\"\n",
    "    cv2.imwrite(file, eroded)\n",
    "    #cv2.imwrite(file, thresh1)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 水平垂直投影"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Projection(file_name):\n",
    "    img=cv2.imread(file_name)  #读取图片，装换为可运算的数组  \n",
    "    GrayImage=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)   #将BGR图转为灰度图  \n",
    "    ret,thresh=cv2.threshold(GrayImage,190,255,cv2.THRESH_BINARY)  #将图片进行二值化（190,255）之间的点均变为255（背景）  \n",
    "\n",
    "    (height,width)=thresh.shape #返回高和宽    \n",
    "    paint = np.zeros(img.shape, np.uint8)\n",
    "    \n",
    "    w = [0 for z in range(0, width)]  \n",
    "    h = [0 for z in range(0, height)]  \n",
    "\n",
    "    #记录每一列的波峰  \n",
    "    for j in range(0,width): #遍历一列   \n",
    "        for i in range(0,height):  #遍历一行  \n",
    "            paint[i,j]=255\n",
    "            if  thresh[i,j]==0:  \n",
    "                w[j]+=1           \n",
    "    for j in range(0,width):  #遍历每一列  \n",
    "        for i in range((height-w[j]),height):  #从该列应该变黑的最顶部的点开始向最底部涂黑  \n",
    "            paint[i,j]=0   #涂黑  \n",
    "    cv2.imwrite(\"images_data/Vertical.png\",paint)\n",
    "\n",
    "    #记录每一行的波峰  \n",
    "    for i in range(0,height):\n",
    "        for j in range(0,width): #遍历一列   \n",
    "            paint[i,j]=255  \n",
    "            if thresh[i,j]==0:   \n",
    "                h[i]+=1        \n",
    "    for i in range(0,height):\n",
    "        for j in range(0,width-(width-h[i])):\n",
    "            paint[i,j]=0   #涂黑  \n",
    "    cv2.imwrite(\"images_data/Horizontal.png\",paint)\n",
    "    return w,h,thresh\n",
    "    # plt.imshow(thresh,cmap=plt.gray())  \n",
    "    # plt.show()  \n",
    "    # cv2.imshow('img',thresh)    \n",
    "    # cv2.waitKey(0)    \n",
    "    # cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 成绩表格识别计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_axis = [] #列\n",
    "y_axis = [] #行\n",
    "Ver = []#垂直投影 len(w)\n",
    "Hor = []#水平投影 len(h)\n",
    "file_name = 'images_data/v1.png'\n",
    "file = Adjust_img(file_name) #切割校正函数 \n",
    "#Ver,Hor,thresh = Projection('images_data/v1.png')\n",
    "Ver,Hor,thresh = Projection(file) #投影统计函数\n",
    "for i in range(len(Hor)-2):\n",
    "    if Hor[i]>850 and Hor[i+1]<850 and Hor[i+2]<850:\n",
    "        x_axis.append(i)\n",
    "for i in range(len(Ver)-2):\n",
    "    if Ver[i]>180 and Ver[i+1]<180 and Ver[i+2]<180:\n",
    "        y_axis.append(i)\n",
    "#print(x_axis,y_axis)\n",
    "\n",
    "mark = 0\n",
    "for i in range(1,len(y_axis)-1):\n",
    "    crop_img = thresh[x_axis[1]+3:x_axis[2]-3,y_axis[i]+3:y_axis[i+1]-3]\n",
    "    cv2.imwrite(\"images_data/img/\"+str(i)+\".png\", crop_img)\n",
    "    ret,thresh1 = cv2.threshold(crop_img,190,255,cv2.THRESH_BINARY_INV)\n",
    "    binary, contours, hierarchy = cv2.findContours( thresh1,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #print(i,len(contours))\n",
    "    a = 0\n",
    "    x0 = 0\n",
    "    for j in range(len(contours)):\n",
    "        x1, y1, w1, h1 = cv2.boundingRect(contours[j])\n",
    "        temp =  crop_img[y1-1:(y1 + h1+1), x1-1:(x1 + w1+1)]\n",
    "        files = \"images_data/img/\" + str(i)+str(j) + \".png\"\n",
    "        cv2.imwrite(files, temp)\n",
    "        imgs = Func.Thumbnail(Image.open(files).convert('1'),28)\n",
    "        result = Func.Getpixels(imgs,0)\n",
    "        prediction=tf.argmax(y_conv,1)\n",
    "        predint=prediction.eval(feed_dict={x: [result],keep_prob: 1.0}, session=sess)#加[]可以将list变为矩阵\n",
    "        #print(files,predint[0])\n",
    "        if a == 0:\n",
    "            a = predint[0]\n",
    "        elif x1>x0:\n",
    "            a = a*10+ predint[0]\n",
    "        else :\n",
    "            a += predint[0]*10\n",
    "        x0 = x1\n",
    "    print(a)\n",
    "    mark += a\n",
    "print(mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片腐蚀膨胀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#运行如果报错，考虑是文件的名称含有中文字符的原因\n",
    "file = 'images_data/ss1.png'\n",
    "img = cv2.imread(file)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY)\n",
    "#ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY_INV) #反色读入\n",
    "#cv2.imwrite(\"images_data/thresh.png\", thresh)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
    "#eroded = cv2.erode(thresh, kernel)\n",
    "dilate = cv2.dilate(thresh,kernel)\n",
    "#cv2.imwrite('images_data/sample.png', eroded)\n",
    "cv2.imwrite('images_data/dilate.png', dilate)\n",
    "kernel1 = cv2.getStructuringElement(cv2.MORPH_RECT, (8, 8))\n",
    "eroded = cv2.erode( dilate, kernel1)\n",
    "cv2.imwrite('images_data/eroded.png', eroded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运算符数据采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_hot(labels):#转换为one_hot模式\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    concated = tf.concat([indices, labels],1)\n",
    "    onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 14]), 1.0, 0.0)\n",
    "    return  onehot_labels\n",
    "\n",
    "def get_files(file_dir):\n",
    "    A0 = []\n",
    "    label_0 = []\n",
    "    for file in os.listdir(file_dir):\n",
    "        A0.append(imageprepare(file_dir+file))\n",
    "        name = file.split(sep='(')\n",
    "        if name[0] == '+ ':\n",
    "            label_0.append(10)\n",
    "        elif name[0] == '- ':\n",
    "            label_0.append(11)\n",
    "        elif name[0] == '× ':\n",
    "            label_0.append(12)\n",
    "        elif name[0] == '÷ ':\n",
    "            label_0.append(13)\n",
    "        else:\n",
    "            name1 = file.split(sep='.')\n",
    "            label_0.append((int(name1[0])%10))\n",
    "        #print(name,label_0[len(label_0)-1]) \n",
    "    return A0 ,label_0#返回两个list        \n",
    "           \n",
    "def imageprepare(file_name):\n",
    "    img = Image.open(file_name).convert('L')#读取一副图像，转化为灰度图像\n",
    "    tv = list(img.getdata()) #get pixel values\n",
    "    #normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [ (255-x)*1.0/255.0 for x in tv] \n",
    "    return tva\n",
    "\n",
    "train_dir = 'images_data/+-x÷/'\n",
    "#存放用来训练的图片的路径\n",
    "image_list,label_list = get_files(train_dir)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(np.array(image_list).shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intro-扩大维度训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "time_start=time.time();#time.time()为1970.1.1到当前时间的毫秒数 \n",
    " \n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "def one_hot(labels):\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    concated = tf.concat([indices, labels],1)\n",
    "    onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 14]), 1.0, 0.0)\n",
    "    return  onehot_labels\n",
    "\n",
    "def random(size):\n",
    "    import random\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    slice = random.sample(range(0,160), size)  #从list中随机获取 个元素，作为一个片断返回  \n",
    "    for i in range(size):\n",
    "        batch_x.append(image_list[slice[i]])\n",
    "        batch_y.append(label_list[slice[i]])\n",
    "    batch_y = one_hot(batch_y)\n",
    "    return batch_x ,batch_y\n",
    "\n",
    "#define placeholder for inputs to netword\n",
    "x = tf.placeholder(\"float\", [None, 784]) \n",
    "y_ = tf.placeholder(\"float\", [None,14])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,14]),dtype = tf.float32,name = 'weights')\n",
    "b = tf.Variable(tf.zeros([14]),dtype = tf.float32,name = 'biases')\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "y_ = tf.placeholder(\"float\", [None,14])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "batch_num = 100\n",
    "\n",
    "for i in range(501):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_num)\n",
    "    img_data,img_label = random(10) #随机取多个样本\n",
    "    train_x = tf.concat([batch_xs,np.array(img_data,dtype=np.float32)],0) \n",
    "    train_y =tf.concat([tf.concat([batch_ys,tf.zeros([batch_num,4])],1),img_label],0)  \n",
    "    test_labels = tf.concat([mnist.test.labels,tf.zeros([10000,4])],1)\n",
    "    \n",
    "    sess.run(train_step, feed_dict={x: sess.run(train_x), y_: sess.run(train_y)})\n",
    "    if i%50 == 0:\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"step %d, training accuracy %g\"%(i, sess.run(accuracy, feed_dict={x:mnist.test.images,y_:sess.run(test_labels)})))  \n",
    "        \n",
    "        \n",
    "saver = tf.train.Saver()   \n",
    "saver.save(sess, \"./model/expand_intro/model.ckpt\")\n",
    "print(b.eval(session=sess))\n",
    "\n",
    "time_end=time.time();#time.time()为1970.1.1到当前时间的毫秒数  \n",
    "print ((time_end-time_start)/60,\"mins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# expert-扩大维度训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "\n",
    "time_start=time.time();#time.time()为1970.1.1到当前时间的毫秒数  \n",
    "\n",
    "sess = tf.InteractiveSession()  \n",
    "def one_hot(labels):\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    concated = tf.concat([indices, labels],1)\n",
    "    onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 14]), 1.0, 0.0)\n",
    "    return  onehot_labels\n",
    "\n",
    "def random(size):\n",
    "    import random\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    slice = random.sample(range(0,210), size)  #从list中随机获取 个元素，作为一个片断返回  \n",
    "    for i in range(size):\n",
    "        batch_x.append(image_list[slice[i]])\n",
    "        batch_y.append(label_list[slice[i]])\n",
    "    batch_y = one_hot(batch_y)\n",
    "    return batch_x ,batch_y\n",
    "\n",
    "#定义一些函数：分配系数函数、分配偏置函数、卷积函数、pooling函数  \n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#均值0标准方差0.1，剔除2倍标准方差之外的随机数据  \n",
    "    return tf.Variable(initial)  \n",
    "    \n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)#统一值0.1  \n",
    "    return tf.Variable(initial)  \n",
    "  \n",
    "def conv2d(x, W):  \n",
    "  #待操作的数据x，模板W，tensor不同维度上的步长，强制与原tensor等大  \n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "  \n",
    "def max_pool_2x2(x):  \n",
    "    #平面数据的pool模板2*2，平面数据滑动步长2*2（非重叠的pool）  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')  \n",
    "  \n",
    "  \n",
    " #x是输入的图像，y_是对应的标签  \n",
    "x = tf.placeholder(tf.float32, [None, 784])  \n",
    "y_ = tf.placeholder(tf.float32, [None, 14])  \n",
    "  \n",
    "#第1层卷积层，Receptive Field 5＊5，单个batch生成32通道数据  \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "  \n",
    "#把图像向量还原成28＊28的图像  \n",
    "x_image = tf.reshape(x, [-1,28,28,1])  \n",
    " \n",
    "#第1个卷积层，使用了ReLU激活函数  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "  \n",
    "#第2层卷积层，Receptive Field 5＊5，单个batch 32通道生成64通道数据  \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "  \n",
    "#第2个卷积层  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "  \n",
    "#全链接层系数  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "  \n",
    "#全链接层：把64通道数据展开方便全链接  \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "  \n",
    "#全链层神经元使用dropout防止过拟合  \n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "  \n",
    "#softmax层系数  \n",
    "W_fc2 = weight_variable([1024, 14])  \n",
    "b_fc2 = bias_variable([14])  \n",
    "  \n",
    "#softmax层  \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "  \n",
    "#交叉熵和训练构型：AdamOptimizer适合这种求和的误差项  \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))  \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  \n",
    "  \n",
    "#验证步骤的构型  \n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  \n",
    "  \n",
    "#初始化  \n",
    "sess.run(tf.global_variables_initializer())  \n",
    "\n",
    "#读取模型继续训练\n",
    "saver = tf.train.Saver() \n",
    "saver.restore(sess, \"./model/expand_expert/model.ckpt-9879\")  \n",
    "\n",
    "max_acc=0\n",
    "t1 = 0\n",
    "batch_num = 100\n",
    "#开始训练  \n",
    "\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "for i in range(1001):  \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_num)\n",
    "    img_data,img_label = random(10) #随机取多个样本\n",
    "    train_x = tf.concat([batch_xs,np.array(img_data,dtype=np.float32)],0) \n",
    "    train_y =tf.concat([tf.concat([batch_ys,tf.zeros([batch_num,4])],1),img_label],0)  \n",
    "    test_labels = tf.concat([mnist.test.labels,tf.zeros([10000,4])],1)\n",
    "      \n",
    "    train_step.run(feed_dict={x:sess.run(train_x), y_:sess.run(train_y), keep_prob: 0.5}) \n",
    "    if i%10 == 0:  \n",
    "        #验证的时候dropout=1.0，训练时=0.5  \n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:sess.run(train_x), y_:sess.run(train_y), keep_prob: 1.0})  \n",
    "        test_accuracy = accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images, y_: sess.run(test_labels), keep_prob: 1.0})\n",
    "        time_end=(time.time()-time_start)/60;\n",
    "        t = time_end - t1\n",
    "        t1 = time_end\n",
    "        print (\"step %d, training accuracy %g,test accuracy %g,time %.01f mins ,%.01f \"%(i, train_accuracy,test_accuracy,time_end,t))\n",
    "        if  test_accuracy>max_acc:\n",
    "            max_acc=test_accuracy\n",
    "            saver.save(sess, \"./model/expand_expert/model.ckpt\",global_step= int( max_acc*10000) ) \n",
    " \n",
    "time_end=time.time();\n",
    "print ((time_end-time_start)/60,\"mins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore intro_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# restore variables\n",
    "# redefine the same shape and same type for your variables\n",
    "W = tf.Variable(np.arange((784*14)).reshape((784, 14)), dtype=tf.float32, name=\"weights\")\n",
    "bias = tf.Variable(np.arange(14), dtype=tf.float32, name=\"biases\")\n",
    "sess = tf.Sessio÷n()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(sess, \"./expand_intro/model.ckpt\")  \n",
    "print(bias.eval(session=sess))\n",
    "print(\"weights:\", sess.run(W))\n",
    "print(\"biases:\", sess.run(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restore expert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "#定义一些函数：分配系数函数、分配偏置函数、卷积函数、pooling函数  \n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#均值0标准方差0.1，剔除2倍标准方差之外的随机数据  \n",
    "    return tf.Variable(initial)  \n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)#统一值0.1  \n",
    "    return tf.Variable(initial)   \n",
    "def conv2d(x, W):  \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')   \n",
    "def max_pool_2x2(x):  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])  \n",
    "y_ = tf.placeholder(tf.float32, [None, 14])  \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "x_image = tf.reshape(x, [-1,28,28,1])  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "W_fc2 = weight_variable([1024, 14])  \n",
    "b_fc2 = bias_variable([14])  \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "#恢复模型\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  \n",
    "saver = tf.train.Saver() \n",
    "saver.restore(sess, \"./model/expand_expert/model.ckpt-9878\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intro-测试自己的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_dir ='img/' #导入自己的图片地址\n",
    "for file in os.listdir(file_dir):\n",
    "    result = Func.imageprepare(file_dir+file)\n",
    "    z = tf.placeholder(\"float\", [1, 784])  #28*28\n",
    "    y1 = tf.nn.softmax(tf.matmul(z,W) + b)\n",
    "    prediction=tf.argmax(y1,1)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    predint=prediction.eval(feed_dict={z: [result]}, session=sess)\n",
    "    print('recognize result:',predint[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# expert-测试自己的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_dir ='images_data/img/' #导入自己的图片地址\n",
    "for file in os.listdir(file_dir):\n",
    "    img = Func.Thumbnail(Image.open(file_dir+file).convert('1'),28)\n",
    "    result = Func.Getpixels(img,0)\n",
    "    prediction=tf.argmax(y_conv,1)\n",
    "    predint=prediction.eval(feed_dict={x: [result],keep_prob: 1.0}, session=sess)#加[]可以将list变为矩阵\n",
    "    print(file,predint[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN读取模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_files(file_dir):\n",
    "    A0 = []\n",
    "    label_0 = []\n",
    "    for file in os.listdir(file_dir):\n",
    "        A0.append(Func.Getpixels(file_dir+file,1))\n",
    "        name = file.split(sep='(')\n",
    "        if name[0] == '+ ':\n",
    "            label_0.append(10)\n",
    "        elif name[0] == '- ':\n",
    "            label_0.append(11)\n",
    "        elif name[0] == '× ':\n",
    "            label_0.append(12)\n",
    "        elif name[0] == '÷ ':\n",
    "            label_0.append(13)\n",
    "        else:\n",
    "            name1 = file.split(sep='.')\n",
    "            label_0.append((int(name1[0])%10))\n",
    "        #print(name,label_0[len(label_0)-1]) \n",
    "    return A0 ,label_0#返回两个list        \n",
    "           \n",
    "train_dir = 'images_data/+-x÷/'\n",
    "#存放用来训练的图片的路径\n",
    "image_list,label_list = get_files(train_dir)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(np.array(image_list).shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "\n",
    "time_start=time.time();#time.time()为1970.1.1到当前时间的毫秒数  \n",
    "\n",
    "sess = tf.InteractiveSession()  \n",
    "\n",
    "#定义一些函数：分配系数函数、分配偏置函数、卷积函数、pooling函数  \n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#均值0标准方差0.1，剔除2倍标准方差之外的随机数据  \n",
    "    return tf.Variable(initial)  \n",
    "    \n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)#统一值0.1  \n",
    "    return tf.Variable(initial)  \n",
    "  \n",
    "def conv2d(x, W):  \n",
    "  #待操作的数据x，模板W，tensor不同维度上的步长，强制与原tensor等大  \n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "  \n",
    "def max_pool_2x2(x):  \n",
    "    #平面数据的pool模板2*2，平面数据滑动步长2*2（非重叠的pool）  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')  \n",
    "  \n",
    "  \n",
    " #x是输入的图像，y_是对应的标签  \n",
    "x = tf.placeholder(tf.float32, [None, 784])  \n",
    "y_ = tf.placeholder(tf.float32, [None, 14])  \n",
    "  \n",
    "#第1层卷积层，Receptive Field 5＊5，单个batch生成32通道数据  \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "  \n",
    "#把图像向量还原成28＊28的图像  \n",
    "x_image = tf.reshape(x, [-1,28,28,1])  \n",
    " \n",
    "#第1个卷积层，使用了ReLU激活函数  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "  \n",
    "#第2层卷积层，Receptive Field 5＊5，单个batch 32通道生成64通道数据  \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "  \n",
    "#第2个卷积层  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "  \n",
    "#全链接层系数  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "  \n",
    "#全链接层：把64通道数据展开方便全链接  \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "  \n",
    "#全链层神经元使用dropout防止过拟合  \n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "  \n",
    "#softmax层系数  \n",
    "W_fc2 = weight_variable([1024, 14])  \n",
    "b_fc2 = bias_variable([14])  \n",
    "  \n",
    "#softmax层  \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "  \n",
    "#交叉熵和训练构型：AdamOptimizer适合这种求和的误差项  \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))  \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  \n",
    "  \n",
    "#验证步骤的构型  \n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))  \n",
    "  \n",
    "#初始化  \n",
    "sess.run(tf.global_variables_initializer())  \n",
    "\n",
    "#读取模型继续训练\n",
    "saver = tf.train.Saver() \n",
    "#saver.restore(sess, \"./model/expand_expert/model.ckpt-9892\")  \n",
    "\n",
    "max_acc=0\n",
    "t1 = 0\n",
    "batch_num = 50\n",
    "#开始训练  \n",
    "for i in range(501):  \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(batch_num)\n",
    "    img_data,img_label = Func.random(15,len(image_list),image_list,label_list) #随机取多个样本\n",
    "    train_x = tf.concat([batch_xs,np.array(img_data,dtype=np.float32)],0) \n",
    "    train_y =tf.concat([tf.concat([batch_ys,tf.zeros([batch_num,4])],1),img_label],0)  \n",
    "    test_labels = tf.concat([mnist.test.labels,tf.zeros([10000,4])],1)\n",
    "      \n",
    "    train_step.run(feed_dict={x:sess.run(train_x), y_:sess.run(train_y), keep_prob: 0.5}) \n",
    "    if i%10 == 0:  \n",
    "        #验证的时候dropout=1.0，训练时=0.5  \n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:sess.run(train_x), y_:sess.run(train_y), keep_prob: 1.0})  \n",
    "        test_accuracy = accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images, y_: sess.run(test_labels), keep_prob: 1.0})\n",
    "        time_end=(time.time()-time_start)/60;\n",
    "        t = time_end - t1\n",
    "        t1 = time_end\n",
    "        print (\"step %d, training accuracy %g,test accuracy %g,time %.01f mins ,%.01f \"%(i, train_accuracy,test_accuracy,time_end,t))\n",
    "        if  test_accuracy>max_acc and i>50:\n",
    "            max_acc=test_accuracy\n",
    "            #saver.save(sess, \"./model/expand_expert/model.ckpt\",global_step= int( max_acc*10000) ) \n",
    "\n",
    "            \n",
    "time_end=time.time();\n",
    "print ((time_end-time_start)/60,\"mins\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data  \n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)  \n",
    "\n",
    "time_start=time.time()\n",
    "sess = tf.InteractiveSession()  \n",
    "  \n",
    "#定义一些函数：分配系数函数、分配偏置函数、卷积函数、pooling函数  \n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#均值0标准方差0.1，剔除2倍标准方差之外的随机数据  \n",
    "    return tf.Variable(initial)  \n",
    "    \n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)#统一值0.1  \n",
    "    return tf.Variable(initial)  \n",
    "  \n",
    "def conv2d(x, W):  \n",
    "  #待操作的数据x，模板W，tensor不同维度上的步长，强制与原tensor等大  \n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "  \n",
    "def max_pool_2x2(x):  \n",
    "    #平面数据的pool模板2*2，平面数据滑动步长2*2（非重叠的pool）  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')  \n",
    "  \n",
    "with tf.name_scope('inputs'):\n",
    "    #x是输入的图像，y_是对应的标签  \n",
    "    x = tf.placeholder(tf.float32, [None, 784],name='x_input')  \n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y_input')  \n",
    "    \n",
    "#把图像向量还原成28＊28的图像  \n",
    "with tf.name_scope('input_reshape1'):  \n",
    "    x_image = tf.reshape(x, [-1,28,28,1]) \n",
    "    \n",
    "#第1层卷积层，Receptive Field 5＊5，单个batch生成32通道数据 使用了ReLU激活函数  \n",
    "with tf.name_scope('conv1'):  \n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "with tf.name_scope('pool1'):  \n",
    "    h_pool1 = max_pool_2x2(h_conv1)  \n",
    "    \n",
    "#第2层卷积池化层，Receptive Field 5＊5，单个batch 32通道生成64通道数据  \n",
    "with tf.name_scope('conv2'):  \n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "    b_conv2 = bias_variable([64]) \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "with tf.name_scope('pool2'):  \n",
    "    h_pool2 = max_pool_2x2(h_conv2)  \n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "#全链接层：把64通道数据展开方便全链接    \n",
    "with tf.name_scope('full_connect'):\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "    b_fc1 = bias_variable([1024])  \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "\n",
    "#全链层神经元使用dropout防止过拟合  \n",
    "with tf.name_scope('dropout'): \n",
    "    keep_prob = tf.placeholder(\"float\")  \n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "\n",
    "#softmax层系数 \n",
    "with tf.name_scope('softmax'):\n",
    "    W_fc2 = weight_variable([1024, 10])  \n",
    "    b_fc2 = bias_variable([10]) \n",
    "    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "tf.summary.histogram( 'outputs', y_conv)\n",
    "with tf.name_scope('result'):\n",
    "    #交叉熵和训练构型：AdamOptimizer适合这种求和的误差项  \n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))  \n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  \n",
    "  \n",
    "    #验证步骤的构型  \n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "#初始化  \n",
    "sess.run(tf.global_variables_initializer())  \n",
    "  \n",
    "#清空文件夹\n",
    "file_dir ='logs/'\n",
    "for file in os.listdir(file_dir):\n",
    "    os.remove(file_dir+file)\n",
    "    \n",
    "    \n",
    "merged =  tf.summary.merge_all()\n",
    "#summary_op = tf.summary.merge([cost_summary, accuracy_summary])\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "#开始训练  \n",
    "for i in range(1000):  \n",
    "    batch = mnist.train.next_batch(50)  \n",
    "    if i%50 == 0:  \n",
    "        #验证的时候dropout=1.0，训练时=0.5  \n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x:batch[0], y_: batch[1], keep_prob: 1.0})  \n",
    "        print (\"step %d, training accuracy %g\"%(i, train_accuracy))  \n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})  \n",
    "    #result = sess.run(merged,feed_dict={xs: x_data, ys: y_data})\n",
    "        #writer.add_summary(result, i)\n",
    "    summary_str = sess.run(merged, feed_dict={x:batch[0], y_:batch[1], keep_prob:(1.0)})\n",
    "    writer.add_summary(summary_str, i)\n",
    "\n",
    "#验证最终的准确率  \n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}) )  \n",
    "\n",
    "#saver = tf.train.Saver()   \n",
    "#saver.save(sess, \"./model/expert/model.ckpt\")  \n",
    "\n",
    "writer.close() ##程序运行结束后关闭文件并刷新到硬盘\n",
    "time_end=time.time();#time.time()为1970.1.1到当前时间的毫秒数  \n",
    "print ((time_end-time_start)/60,\"mins\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir=D://Jupyter//logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "头文件部分\n",
    "from PIL import Image, ImageFilter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import Func\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "读取模型部分\n",
    "#定义一些函数：分配系数函数、分配偏置函数、卷积函数、pooling函数  \n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#均值0标准方差0.1，剔除2倍标准方差之外的随机数据  \n",
    "    return tf.Variable(initial)  \n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)#统一值0.1  \n",
    "    return tf.Variable(initial)   \n",
    "def conv2d(x, W):  \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')   \n",
    "def max_pool_2x2(x):  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')  \n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])  \n",
    "y_ = tf.placeholder(tf.float32, [None, 14])  \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "x_image = tf.reshape(x, [-1,28,28,1])  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)  \n",
    "h_pool2 = max_pool_2x2(h_conv2)  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])  \n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "keep_prob = tf.placeholder(\"float\")  \n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)  \n",
    "W_fc2 = weight_variable([1024, 14])  \n",
    "b_fc2 = bias_variable([14])  \n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)  \n",
    "#恢复模型\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())  \n",
    "saver = tf.train.Saver() \n",
    "saver.restore(sess, \"./model/expand_expert/model.ckpt-9892\")  \n",
    "\n",
    "\n",
    "对文件夹内的每个图片进行识别的部分\n",
    "\n",
    "#清空文件夹\n",
    "file_dir ='images_data/img/'\n",
    "for file in os.listdir(file_dir):\n",
    "    os.remove(file_dir+file)\n",
    "    \n",
    "\n",
    "file_name='images_data/formula/formula2.png'\n",
    "img = cv2.imread(file_name)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "#120  124 150 3\n",
    "ret,thresh = cv2.threshold(gray,120,255,cv2.THRESH_BINARY)\n",
    "#ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY_INV) #反色读入\n",
    "#cv2.imwrite(\"images_data/img/thresh.png\", thresh)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4,4))\n",
    "dilate = cv2.dilate(thresh,kernel)\n",
    "eroded = cv2.erode(dilate, kernel)\n",
    "#cv2.imwrite('images_data/img/dilate.png', dilate)\n",
    "file1 = 'images_data/img/eroded.png'\n",
    "cv2.imwrite(file1, eroded)\n",
    "eroded = cv2.imread(file1)\n",
    "\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original picture\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "plt.imshow(eroded)\n",
    "plt.title(\"Binary picture\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "im = Image.open(file1).convert('1')\n",
    "w,h = im.size\n",
    "edge = [0,0,0,0] #上下左右\n",
    "lock = [0,0]\n",
    "cnt = [0,0]\n",
    "num = []\n",
    "for i in range(h): #每一行\n",
    "    if Func.Judgezero(im,i,0)==1 and lock[0]==0:\n",
    "        edge[0],lock[0] = i,1\n",
    "    if Func.Judgezero(im,i,0)==0 and lock[0]==1:\n",
    "        edge[1],lock[0] = i,0\n",
    "        cnt[0] += 1\n",
    "        #print(edge[0],edge[1])\n",
    "        img0 = im.crop((0,edge[0],w,edge[1]))\n",
    "        img0.save(file_dir+\"img\"+str(cnt[0])+\".png\") \n",
    "        cnt[1]= 0\n",
    "        for j in range(w):# 每一列\n",
    "            if Func.Judgezero(img0,j,1)==1 and lock[1]==0:\n",
    "                edge[2],lock[1] = j,1\n",
    "            if Func.Judgezero(img0,j,1)==0 and lock[1]==1:\n",
    "                edge[3],lock[1] = j,0\n",
    "                cnt[1] += 1\n",
    "                #print(edge[2],edge[3])\n",
    "                img1 = img0.crop((edge[2],0,edge[3],edge[1]-edge[0]))\n",
    "                #print((img1.crop(Func.JudgeEdge(img1)).size))\n",
    "                img2 = Func.Thumbnail(img1.crop(Func.JudgeEdge(img1)),28)\n",
    "                img2.save(file_dir+\"img\"+str(cnt[0])+'%02d' %cnt[1]+\".png\")    \n",
    "        \n",
    "        num.append(cnt[1])\n",
    "print(num)\n",
    "\n",
    "\n",
    "for i in range(len(num)):\n",
    "    ans = []\n",
    "    for j in range(num[i]):\n",
    "        img = Func.Thumbnail(Image.open(file_dir+\"img\"+str(i+1)+'%02d' %(j+1)+\".png\").convert('1'),28)\n",
    "        plt.subplot(1,num[i],j+1)\n",
    "        plt.imshow(img)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        result = Func.Getpixels(img,0)\n",
    "        prediction=tf.argmax(y_conv,1)\n",
    "        predint=prediction.eval(feed_dict={x: [result],keep_prob: 1.0}, session=sess)#加[]可以将list变为矩阵\n",
    "        if predint[0] == 10:\n",
    "            ans.append(\"+\")\n",
    "        elif predint[0] == 11:\n",
    "            ans.append(\"-\")\n",
    "        elif predint[0] == 12:\n",
    "            ans.append(\"*\")\n",
    "        elif predint[0] == 13:\n",
    "            ans.append(\"/\")\n",
    "        else :\n",
    "            ans.append(predint[0])\n",
    "    plt.show()\n",
    "    answer = \"\".join('%s' %id for id in ans)\n",
    "    #print('list:',ans) \n",
    "    #print('string:',answer)\n",
    "    print(\"answer:\",answer,\"=\",eval(answer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir ='images_data/num/' #导入自己的图片地址\n",
    "for file in os.listdir(file_dir):\n",
    "    ans = []\n",
    "    img = Func.Thumbnail(Image.open(file_dir+file).convert('1'),28)\n",
    "    #plt.imshow(img)\n",
    "    #plt.xticks([])\n",
    "    #plt.yticks([])\n",
    "    #plt.show()\n",
    "    result = Func.Getpixels(img,0)\n",
    "    prediction=tf.argmax(y_conv,1)\n",
    "    predint=prediction.eval(feed_dict={x: [result],keep_prob: 1.0}, session=sess)#加[]可以将list变为矩阵\n",
    "    if predint[0] == 10:\n",
    "        ans.append(\"＋\")\n",
    "    elif predint[0] == 11:\n",
    "        ans.append(\"－\")\n",
    "    elif predint[0] == 12:\n",
    "        ans.append(\"×\")\n",
    "    elif predint[0] == 13:\n",
    "        ans.append(\"÷\")\n",
    "    else :\n",
    "        ans.append(predint[0])\n",
    "    #file=file.PadLeft(20, \" \");\n",
    "    print(\"file_name:\",file,\"   result:\",ans[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score  Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Adjust_img(file_name):\n",
    "    img = cv2.imread(file_name)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Original picture\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    result = img.copy()\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY)\n",
    "    ret,thresh = cv2.threshold(gray,150,255,cv2.THRESH_BINARY_INV) #反色读入\n",
    "\n",
    "    #binary, contours, hierarchy = cv2.findContours(eroded,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    binary, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "    temp = result[y:(y + h), x:(x + w)]\n",
    "    rect = cv2.minAreaRect(contours[0])\n",
    "    height,width = temp.shape[:2]\n",
    "\n",
    "    up = int((height - rect[1][0])/2)\n",
    "    down = int(height - up)\n",
    "    left = int((width - rect[1][1])/2)\n",
    "    right =int(width - left)\n",
    "    #第一个参数旋转中心，第二个参数旋转角度，第三个参数：缩放比例\n",
    "    M = cv2.getRotationMatrix2D((width/2,height/2),(rect[2]+90),1)\n",
    "    res = cv2.warpAffine(temp,M,(width,height))\n",
    "    temp1 = res[up:down,left:right]\n",
    "    gray1 = cv2.cvtColor(temp1,cv2.COLOR_BGR2GRAY)\n",
    "    ret,thresh1 = cv2.threshold(gray1,190,255,cv2.THRESH_BINARY)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    eroded = cv2.erode(thresh1, kernel)\n",
    "    \n",
    "    file = \"images_data/img/adjust.png\"\n",
    "    cv2.imwrite(file, eroded)\n",
    "    #plt.tilte('Title in a custom color',color='#123456'）  \n",
    "    plt.imshow(eroded)\n",
    "    plt.title(\"Correction picture\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    #cv2.imwrite(file, thresh1)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Projection(file_name):\n",
    "    img=cv2.imread(file_name)  #读取图片，装换为可运算的数组  \n",
    "    GrayImage=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)   #将BGR图转为灰度图  \n",
    "    ret,thresh=cv2.threshold(GrayImage,190,255,cv2.THRESH_BINARY)  #将图片进行二值化（190,255）之间的点均变为255（背景）  \n",
    "    cv2.imwrite(\"images_data/img/adjust.png\", thresh)\n",
    "    (height,width)=thresh.shape #返回高和宽    \n",
    "    paint = np.zeros(img.shape, np.uint8)\n",
    "    \n",
    "    w = [0 for z in range(0, width)]  \n",
    "    h = [0 for z in range(0, height)]  \n",
    "\n",
    "    #记录每一列的波峰  \n",
    "    for j in range(0,width): #遍历一列   \n",
    "        for i in range(0,height):  #遍历一行  \n",
    "            paint[i,j]=255\n",
    "            if  thresh[i,j]==0:  \n",
    "                w[j]+=1           \n",
    "    for j in range(0,width):  #遍历每一列  \n",
    "        for i in range((height-w[j]),height):  #从该列应该变黑的最顶部的点开始向最底部涂黑  \n",
    "            paint[i,j]=0   #涂黑  \n",
    "    cv2.imwrite(\"images_data/img/Vertical.png\",paint)\n",
    "    plt.imshow(paint)\n",
    "    plt.title(\"Vertical projection\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "    #记录每一行的波峰  \n",
    "    for i in range(0,height):\n",
    "        for j in range(0,width): #遍历一列   \n",
    "            paint[i,j]=255  \n",
    "            if thresh[i,j]==0:   \n",
    "                h[i]+=1        \n",
    "    for i in range(0,height):\n",
    "        for j in range(0,width-(width-h[i])):\n",
    "            paint[i,j]=0   #涂黑  \n",
    "    cv2.imwrite(\"images_data/img/Horizontal.png\",paint)\n",
    "    plt.imshow(paint)  \n",
    "    plt.title(\"Horizontal projection\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()  \n",
    "    return w,h,thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score  Recognition2（非投影方式）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清空文件夹\n",
    "file_dir ='images_data/img/'\n",
    "for file in os.listdir(file_dir):\n",
    "    os.remove(file_dir+file)\n",
    "    \n",
    "\n",
    "file_name='images_data/score/score4.png'\n",
    "img = cv2.imread(file_name)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "ret,thresh = cv2.threshold(gray,180,255,cv2.THRESH_BINARY)\n",
    "#ret,thresh = cv2.threshold(gray,190,255,cv2.THRESH_BINARY_INV) #反色读入\n",
    "#cv2.imwrite(\"images_data/img/thresh.png\", thresh)\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "dilate = cv2.dilate(thresh,kernel)\n",
    "eroded = cv2.erode(dilate, kernel)\n",
    "#cv2.imwrite('images_data/img/dilate.png', dilate)\n",
    "file1 = 'images_data/img/eroded.png'\n",
    "cv2.imwrite(file1, eroded)\n",
    "eroded = cv2.imread(file1)\n",
    "\n",
    "#plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original picture\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "#plt.subplot(1,2,2)\n",
    "plt.imshow(eroded)\n",
    "plt.title(\"Binary picture\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "im = Image.open(file1).convert('1')\n",
    "w,h = im.size\n",
    "\n",
    "img= im.crop(Func.JudgeEdge(im))\n",
    "img.save('sample.png')        \n",
    "w,h = img.size\n",
    "A = []\n",
    "mark = 0\n",
    "\n",
    "\n",
    "\n",
    "img1 = cv2.imread('sample.png')\n",
    "gray = cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "start = [0.155,0.26,0.365,0.47,0.578,0.685,0.79]\n",
    "end = [0.245,0.35,0.46,0.57,0.675,0.78,0.87] \n",
    "for i in range(1,8):\n",
    "    crop_img = gray[int(0.36*h):int(0.625*h),int(start[i-1]*w):int(end[i-1]*w)]\n",
    "    plt.subplot(1,7,i)\n",
    "    plt.imshow(crop_img)  \n",
    "    #plt.axis('off')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    cv2.imwrite(\"images_data/img/\"+str(i)+\".png\", crop_img)\n",
    "    \n",
    "    ret,thresh1 = cv2.threshold(crop_img,140,255,cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    binary, contours, hierarchy = cv2.findContours(thresh1,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #print(i,len(contours))\n",
    "    a = 0\n",
    "    x0 = 0\n",
    "    for j in range(len(contours)):\n",
    "        x1, y1, w1, h1 = cv2.boundingRect(contours[j])\n",
    "        temp =  crop_img[y1-1:(y1 + h1+1), x1-1:(x1 + w1+1)]\n",
    "        files = \"images_data/img/\" + str(i)+str(j) + \".png\"\n",
    "        cv2.imwrite(files, temp)\n",
    "        imgs = Func.Thumbnail(Image.open(files).convert('1'),28)\n",
    "        result = Func.Getpixels(imgs,0)\n",
    "        prediction=tf.argmax(y_conv,1)\n",
    "        predint=prediction.eval(feed_dict={x: [result],keep_prob: 1.0}, session=sess)#加[]可以将list变为矩阵\n",
    "        #print(files,predint[0])\n",
    "        if a == 0:\n",
    "            a = predint[0]\n",
    "        elif x1>x0:\n",
    "            a = a*10+ predint[0]\n",
    "        else :\n",
    "            a += predint[0]*10\n",
    "        x0 = x1\n",
    "    A.append(a)\n",
    "    #print(a)\n",
    "    mark += a\n",
    "  \n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show() \n",
    "print(A)\n",
    "print(\"ans =\" ,mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 笔记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "重启内核会使运算加快\n",
    "读取模型需要重启内核\n",
    "截取图片要把图片放到中间，四周留出一定的空白 识别率会高一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.concat([W,batch_ys],1) #连接两个tensor\n",
    "print('c.shape:',c.shape)#显示tensor格式\n",
    "tf.reshape(100,14)#格式变化，但是数据总量不变\n",
    "print(c.eval(session=sess))#显示tensor的数据\n",
    "print(y1.eval(feed_dict={z: [result]}, session=sess))\n",
    "\n",
    "plt.imshow(im)  \n",
    "plt.title(\"Original picture\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "img = cv2.imread( file_name)#读取图片\n",
    "img.shape #图片的size\n",
    "img = np.zeros([64,64], np.uint8)  #cv2新建图片\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #转换为灰度图\n",
    "ret,thresh = cv2.threshold(img,190,255,cv2.THRESH_BINARY_INV)#二值化\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (10, 10))#腐蚀内核\n",
    "eroded = cv2.erode(gray, kernel)#腐蚀图片\n",
    "image = img.copy()  #复制图片\n",
    "cv2.imwrite('images/thresh2.png', thresh)#保存图片\n",
    "img=cv2.resize(res,(width,height),interpolation=cv2.INTER_CUBIC)#改变图片尺寸大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=[[1,2],[2,3],[3,4]]\n",
    "b=[[1,2,3], [2,3,4], [3,4,5]]\n",
    "print(np.hstack((a, b)))#数组组合\n",
    "\n",
    "d = np.array([[1, 2], [5, 6], [9, 10]], dtype=np.float) #何强制生成一个 float 类型的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #one_hot数据格式转化\n",
    "sess = tf.Session()\n",
    "labels = [12,13,5,7,9]\n",
    "batch_size = tf.size(labels)\n",
    "labels = tf.expand_dims(labels, 1)\n",
    "indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "concated = tf.concat([indices, labels],1)\n",
    "onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 14]), 1.0, 0.0)\n",
    "sess.run(onehot_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "读取模型要注意先重启内核！！\n",
    "注意保存和读取程序不要在一个文档里运行，容易报错，最好在两个里单独运行，\n",
    "而且读取出错误可以重启kernel来再次运行，运行中发现连续两次运行读取程序，\n",
    "第一次运行结果是正常保存的值，而第二次运行的就是初始化的零值，\n",
    "第三次又是正常值，读取次数多了就会报错了\n",
    "恢复模型后在print（b）过程中不要初始化，否则会导致sess运行的是初始化的值，即会出现全为零的情况\n",
    "'''\n",
    "import tensorflow as tf  \n",
    "import numpy as np\n",
    "#Save to file\n",
    "#remember to define the same dtype and shape when restore\n",
    "W = tf.Variable([[9,4,3,5],[5,4,7,8]],dtype = tf.float32,name = 'weights')\n",
    "b = tf.Variable([[3,6,9]],dtype = tf.float32,name = 'biases')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    save_path = saver.save(sess,\"my_net/save_net.ckpt\")\n",
    "    print(\"Save to path:\",save_path)\n",
    "    \n",
    "################################################\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# restore variables\n",
    "# redefine the same shape and same type for your variables\n",
    "W = tf.Variable(np.arange(8).reshape((2, 4)), dtype=tf.float32, name=\"weights\")\n",
    "b = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name=\"biases\")\n",
    "\n",
    "# not need init step\n",
    "#saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"my_net/save_net.ckpt\")\n",
    "    print(\"weights:\", sess.run(W))\n",
    "    print(\"biases:\", sess.run(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func.py函数（需要保存成py文件再运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "from PIL import Image, ImageFilter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import Func\n",
    "import time\n",
    "import os\n",
    "\n",
    "#读取图像转化为标准尺寸，返回图片的像素列表\n",
    "def Getpixels(im,flag): #flag = 1 为地址\n",
    "    if flag==1:\n",
    "        im = Image.open(im).convert('L')#读取一副图像，转化为灰度图像\n",
    "    tv = list(im.getdata()) #get pixel values\n",
    "    tva = [ (255-x)*1.0/255.0 for x in tv] \n",
    "    return tva\n",
    "\n",
    "#判断某一列全为零 返回值为0/1 全零为0 非全零为1\n",
    "def Judgezero(img,num,flag):#判断某一行、列是否全为零 全零为0  flag 0 行判断 1 列判断\n",
    "    pixs = np.asarray(img)\n",
    "    if flag == 0: #判断一行\n",
    "        result = 1 if list(pixs[num,:]==0).count(1) > 3 else 0\n",
    "    else : #判断一列\n",
    "        result = 1 if list(pixs[:,num]==0).count(1) > 3 else 0\n",
    "    return result\n",
    "\n",
    "def JudgeEdge(img): \n",
    "    pixs = np.asarray(img)\n",
    "    size = [0,0,0,0]\n",
    "    #寻找上下边界 \n",
    "    for i in range(len(pixs)):\n",
    "        if size[1]==0:\n",
    "            if Judgezero(pixs,i,0) == 1:\n",
    "                size[1] = i\n",
    "        if size[3]==0:\n",
    "            if Judgezero(pixs,len(pixs)-i-1,0) == 1:\n",
    "                size[3]=len(pixs)-i-1 \n",
    "        if size[1] != 0 and size[3] != 0:\n",
    "            break\n",
    "    #寻找左右边界\n",
    "    for i in range(len(pixs[0])):\n",
    "        if size[0]==0:\n",
    "            if Judgezero(pixs,i,1) == 1:\n",
    "                size[0] = i\n",
    "        if size[2]==0:\n",
    "            if Judgezero(pixs,len(pixs[0])-i-1,1) == 1:\n",
    "                size[2]=len(pixs[0])-i-1 \n",
    "        if size[0] != 0 and size[2] != 0:\n",
    "            break\n",
    "    return size\n",
    "\n",
    "def Thumbnail(img,leng):\n",
    "    \n",
    "    width = float(img.size[0]) \n",
    "    height = float(img.size[1]) \n",
    "    newImage = Image.new('1', (leng, leng), (255)) # creates white canvas of 28x28 pixels \n",
    "    if width > height: # check which dimension is bigger \n",
    "        # Width is bigger. Width becomes 20 pixels. \n",
    "        nheight = int(round(((leng-4) / width * height), 0)) # resize height according to ratio width \n",
    "        if (nheight <5): # rare case but minimum is 1 pixel \n",
    "            nheigth = 5\n",
    "            # resize and sharpen\n",
    "        img = img.resize(((leng-4), nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN) \n",
    "        wtop = int(round(((leng - nheight) / 2), 0)) # caculate horizontal pozition \n",
    "        newImage.paste(img, (2, wtop)) # paste resized image on white canvas \n",
    "    else: \n",
    "        # Height is bigger. Heigth becomes 20 pixels. \n",
    "        nwidth = int(round(((leng-4) / height * width), 0)) # resize width according to ratio height \n",
    "        if (nwidth <5): # rare case but minimum is 1 pixel \n",
    "            nwidth = 5\n",
    "            # resize and sharpen \n",
    "        img = img.resize((nwidth, leng-4), Image.ANTIALIAS).filter(ImageFilter.SHARPEN) \n",
    "        wleft = int(round(((leng - nwidth) / 2), 0)) # caculate vertical pozition \n",
    "        newImage.paste(img, (wleft, 2)) # paste resized image on white canvas \n",
    "    return newImage\n",
    "\n",
    "def one_hot(labels):#转换为one_hot模式\n",
    "    batch_size = tf.size(labels)\n",
    "    labels = tf.expand_dims(labels, 1)\n",
    "    indices = tf.expand_dims(tf.range(0, batch_size, 1), 1)\n",
    "    concated = tf.concat([indices, labels],1)\n",
    "    onehot_labels = tf.sparse_to_dense(concated, tf.stack([batch_size, 14]), 1.0, 0.0)\n",
    "    return  onehot_labels\n",
    "\n",
    "def random(size,rge,image_list,label_list):\n",
    "    import random\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    slice = random.sample(range(0,rge), size)  #从list中随机获取 个元素，作为一个片断返回  \n",
    "    for i in range(size):\n",
    "        batch_x.append(image_list[slice[i]])\n",
    "        batch_y.append(label_list[slice[i]])\n",
    "    #batch_y = Func.one_hot(batch_y)\n",
    "    batch_y = one_hot(batch_y)\n",
    "    return batch_x ,batch_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

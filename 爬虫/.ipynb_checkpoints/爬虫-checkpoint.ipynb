{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图片爬虫：爬取淘宝的图片\n",
    "\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "keyname=\"\"   \n",
    "key=urllib.request.quote(keyname) \n",
    "\n",
    "\n",
    "for i in range(0,2):   \n",
    "    url=\"https://s.taobao.com/list?spm=a21bo.2017.201867-links-0.4.5af911d9xiD1Dh&q=\"+key+\"&cat=16&seller_type=taobao&oetag=6745&source=qiangdiao\"+str(i*60)\n",
    "    print(url)\n",
    "    print(\"\")\n",
    "    data=urllib.request.urlopen(url).read().decode(\"utf-8\",\"ignore\")\n",
    "    \n",
    "    pat='\"pic_url\":\"(.*?).jpg'\n",
    "    imagelist=re.compile(pat).findall(data) #图片的网站\n",
    "    print(imagelist)\n",
    "\n",
    "    #下面循环爬取每一页中所有的图片\n",
    "    for j in range(0,len(imagelist)):\n",
    "        thisimg=imagelist[j]\n",
    "        thisimgurl=\"http://\"+thisimg+\".jpg\" \n",
    "        file=\"imgs/\"+str(i)+\"-\"+str(j)+\".jpg\"\n",
    "        urllib.request.urlretrieve(thisimgurl,filename=file)\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "response = urllib.request.urlopen('https://www.python.org')\n",
    "#print(response. read(). decode ('utf-8'))\n",
    "print(type(response))\n",
    "print(response.status)\n",
    "print(response.getheaders())\n",
    "print(response.getheader('Content-Type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "\n",
    "request = urllib.request.Request('http://e1.wkcsncjdbd.top/pw/html_data/14/1903/3953948.html')\n",
    "response = urllib.request.urlopen(request)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/58.0.3029.110 Safari/537.36'}\n",
    "\n",
    "r = requests.get(\"http://e1.wkcsncjdbd.top/pw/thread.php?fid=14\",headers=headers)\n",
    "#print(r.text)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import re  \n",
    "from bs4 import BeautifulSoup\n",
    "# 发出请求获得HTML源码\n",
    "#url = 'http://e1.wkcsncjdbd.info/pw/thread.php?fid=21'\n",
    "url = 'http://e1.wkcsncjdbd.info/pw/html_data/21/1903/3954318.html'\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "# 代理，免费的代理只能维持一会可能就没用了，自行更换\n",
    "resp = requests.get(url, headers=headers)\n",
    "\n",
    "#main > div:nth-child(10) > span.pages > span > a\n",
    "\n",
    "# BeautifulSoup解析页面得到最高页码数\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "#print(soup)\n",
    "\n",
    "#links = soup.find_all('03.13' ,src=re.compile(r'.html$'))\n",
    "#print(links)\n",
    "# 获得最高页码数\n",
    "#allpage = soup.find(' > span.pages > span > a')\n",
    "allpage = soup.find_all(class_=\"f_one f1\")\n",
    "#allpage = soup.find_all('#a_ajax_')\n",
    "#allpage = soup.select('> span.pages.fl > span > a')\n",
    "\n",
    "#a_ajax_3951582\n",
    "#a_ajax_3951581\n",
    "print(allpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request  \n",
    "from  bs4 import BeautifulSoup  \n",
    "import re  \n",
    "import time  \n",
    "  \n",
    "rl = \"https://www.zhihu.com/question/22918070\"  \n",
    "html = request.urlopen(url).read().decode('utf-8')  \n",
    "soup = BeautifulSoup(html,'html.parser')  \n",
    "#print(soup.prettify())  \n",
    "  \n",
    "#用Beautiful Soup结合正则表达式来提取包含所有图片链接（img标签中，class=**，以.jpg结尾的链接）的语句  \n",
    "links = soup.find_all('img', \"origin_image zh-lightbox-thumb\",src=re.compile(r'.jpg$'))  \n",
    "# 设置保存图片的路径，否则会保存到程序当前路径  \n",
    "path = r'D:\\Python\\test\\images'                            #路径前的r是保持字符串原始值的意思，就是说不对其中的符号进行转义  \n",
    "for link in links:  \n",
    "    print(link.attrs['src'])  \n",
    "    #保存链接并命名，time.time()返回当前时间戳防止命名冲突  \n",
    "    request.urlretrieve(link.attrs['src'],path+'\\%s.jpg' % time.time())  #使用request.urlretrieve直接将所有远程链接数据下载到本地 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    target = 'http://www.biqukan.com/1_1094/5403177.html'\n",
    "    req = requests.get(url=target)\n",
    "    print(req.text)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     target = 'http://www.biqukan.com/1_1094/17939510.html'\n",
    "#     req = requests.get(url = target)\n",
    "#     html = req.text\n",
    "#     bf = BeautifulSoup(html)\n",
    "#     texts = bf.find_all('div', class_ = 'showtxt') \n",
    "#     print(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "if __name__ == \"__main__\":\n",
    "     target = 'http://www.biqukan.com/1_1094/'\n",
    "     req = requests.get(url = target)\n",
    "     html = req.text\n",
    "     div_bf = BeautifulSoup(html)\n",
    "     div = div_bf.find_all('div', class_ = 'listmain')\n",
    "     print(div[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:UTF-8 -*-\n",
    "import requests\n",
    "if __name__ == '__main__':\n",
    "     target = 'https://unsplash.com/'\n",
    "     req = requests.get(url=target)\n",
    "     print(req.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "resp = requests.get('http://www.baidu.com/img/baidu_jgylogo3.gif',headers=headers)\n",
    "print(resp.content) # 二进制文件使用content\n",
    "# 保存图片\n",
    "with open('logo.gif','wb') as f:\n",
    "    f.write(resp.content)\n",
    "    print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "resp = requests.get('http://www.baidu.com',headers = headers)#headers 来模仿浏览器信息\n",
    "print(resp.text[:20]+'...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "response = requests.get('http://p3.csgfnmdb.com/pw/html_data/14/1903/3953947.html')\n",
    "print(response.status_code) # 状态码\n",
    "#print(response.text) # 网页源码\n",
    "print(response.headers) # 头部信息\n",
    "print(response.cookies) # Cookie\n",
    "print(response.url)# 请求的url\n",
    "print(response.history) # 访问的历史记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "response = requests.get('http://www.baidu.com/')\n",
    "exit() if not resp.status_code == 200 else print('Sucessful')\n",
    "bad_r = requests.get('http://httpbin.org/status/404')\n",
    "print(bad_r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "url = 'http://jandan.net/ooxx/'\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "resp = requests.get(url, headers=headers)\n",
    " \n",
    "soup = BeautifulSoup(resp.text, 'lxml')b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#  author: yukun\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "    \n",
    "# 发出请求获得HTML源码的函数\n",
    "def get_html(url):\n",
    "    # 伪装成浏览器访问\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    resp = requests.get(url, headers=headers).text\n",
    " \n",
    "    return resp\n",
    " \n",
    "# 解析页面，获得数据信息\n",
    "def html_parse():\n",
    "    # 调用函数，for循环迭代出所有页面\n",
    "    for url in all_page():\n",
    "        # BeautifulSoup的解析\n",
    "        soup = BeautifulSoup(get_html(url), 'lxml')\n",
    "        # 书名\n",
    "        alldiv = soup.find_all('div', class_='pl2')\n",
    "        names = [a.find('a')['title'] for a in alldiv]\n",
    "        # 作者\n",
    "        allp = soup.find_all('p', class_='pl')\n",
    "        authors = [p.get_text() for p in allp]\n",
    "        # 评分\n",
    "        starspan = soup.find_all('span', class_='rating_nums')\n",
    "        scores = [s.get_text() for s in starspan]\n",
    "        # 简介\n",
    "        sumspan = soup.find_all('span', class_='inq')\n",
    "        sums = [i.get_text() for i in sumspan]\n",
    "        for name, author, score, sum in zip(names, authors, scores, sums):\n",
    "            name = '书名：' + str(name) + '\\n'\n",
    "            author = '作者：' + str(author) + '\\n'\n",
    "            score = '评分：' + str(score) + '\\n'\n",
    "            sum = '简介：' + str(sum) + '\\n'\n",
    "            data = name + author + score + sum\n",
    "            # 保存数据\n",
    "            f.writelines(data + '=======================' + '\\n')\n",
    " \n",
    "# 获得所有页面的函数\n",
    "def all_page():\n",
    "    base_url = 'https://book.douban.com/top250?start='\n",
    "    urllist = []\n",
    "    # 从0到225，间隔25的数组\n",
    "    for page in range(0, 250, 25):\n",
    "        allurl = base_url + str(page)\n",
    "        urllist.append(allurl)\n",
    " \n",
    "    return  urllist\n",
    " \n",
    "# 文件名\n",
    "filename = '豆瓣图书Top250.txt'\n",
    "# 保存文件操作\n",
    "f = open(filename, 'w', encoding='utf-8')\n",
    "# 调用函数\n",
    "html_parse()\n",
    "f.close()\n",
    "print('保存成功。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#  author: yukun\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "\n",
    "# 发出请求获得HTML源码\n",
    "def get_html(url):\n",
    "    # 指定一个浏览器头\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    # 代理，免费的代理只能维持一会可能就没用了，自行更换\n",
    "    proxies = {'http': '111.23.10.27:8080'}\n",
    "    try:\n",
    "        # Requests库的get请求\n",
    "        resp = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        # 如果请求被阻，就使用代理\n",
    "        resp = requests.get(url, headers=headers, proxies=proxies)\n",
    "        print('代理登陆中')\n",
    " \n",
    "    return resp\n",
    " \n",
    "# 创建文件夹的函数，保存到D盘\n",
    "def mkdir(path):\n",
    "    # os.path.exists(name)判断是否存在路径\n",
    "    # os.path.join(path, name)连接目录与文件名\n",
    "    isExists = os.path.exists(os.path.join(\"E:\\jiandan\", path))\n",
    "    # 如果不存在\n",
    "    if not isExists:\n",
    "        print('makedir', path)\n",
    "        # 创建文件夹\n",
    "        os.makedirs(os.path.join(\"E:\\jiandan\", path))\n",
    "        # 切换到创建的文件夹\n",
    "        os.chdir(os.path.join(\"E:\\jiandan\", path))\n",
    "        return True\n",
    "    # 如果存在了就返回False\n",
    "    else:\n",
    "        print(path, 'already exists')\n",
    "        return False\n",
    "\n",
    "# 获得图片地址调用download函数进行下载\n",
    "def get_imgs():\n",
    "    # 调用函数获得所有页面\n",
    "    for url in all_page():\n",
    "        path = url.split('-')[-1]\n",
    "        # 创建文件夹的函数\n",
    "        mkdir(path)\n",
    "        # 调用请求函数获得HTML源码\n",
    "        html = get_html(url).text\n",
    "        # 使用lxml解析器，也可以使用html.parser\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        # css选择器\n",
    "        #allimgs = soup.select('div.text &gt; p &gt; img')\n",
    "        allimgs = soup.select('div > div > div.text > p > img')\n",
    "        #allimgs = soup.select('#comment')\n",
    "        \n",
    "        print(allimgs)\n",
    "        \n",
    "        # 调用download函数下载保存\n",
    "        download(allimgs)\n",
    "    # 执行完毕打出ok\n",
    "    print('ok')\n",
    "\n",
    "# 获得所有页面\n",
    "def all_page():\n",
    "    base_url = 'http://jandan.net/ooxx/'\n",
    "    # BeautifulSoup解析页面得到最高页码数\n",
    "    soup = BeautifulSoup(get_html(base_url).text, 'lxml')\n",
    "    # 获得最高页码数\n",
    "    allpage = soup.find('span', class_=\"current-comment-page\").get_text()[1:-1]\n",
    "    urllist = []\n",
    "    # for循环迭代出所有页面，得到url\n",
    "    #for page in range(1, int(allpage)+1):\n",
    "    for page in range(1, 2):\n",
    "        allurl = base_url + 'page-' + str(page)\n",
    "        urllist.append(allurl)\n",
    "    return urllist\n",
    " \n",
    "# 保存图片函数，传入的参数是一页所有图片url集合\n",
    "def download(list):\n",
    "    for img in list:\n",
    "        urls = img['src']\n",
    "        # 判断url是否完整\n",
    "        if urls[0:5] == 'http:':\n",
    "            img_url = urls\n",
    "        else:\n",
    "            img_url = 'http:' + urls\n",
    "        filename = img_url.split('/')[-1]\n",
    "        # 保存图片\n",
    "        with open(filename, 'wb') as f:\n",
    "            # 直接过滤掉保存失败的图片，不终止程序\n",
    "            try:\n",
    "                f.write(get_html(img_url).content)\n",
    "                print('Sucessful image:',filename)\n",
    "            except:\n",
    "                print('Failed:',filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 计时\n",
    "    t1 = time.time()\n",
    "    # 调用函数\n",
    "    get_imgs()\n",
    "    print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#  author: yukun\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "\n",
    "# 发出请求获得HTML源码\n",
    "def get_html(url):\n",
    "    # 指定一个浏览器头\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    # 代理，免费的代理只能维持一会可能就没用了，自行更换\n",
    "    proxies = {'http': '111.23.10.27:8080'}\n",
    "    try:\n",
    "        # Requests库的get请求\n",
    "        resp = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        # 如果请求被阻，就使用代理\n",
    "        resp = requests.get(url, headers=headers, proxies=proxies)\n",
    "        print('代理登陆中')\n",
    "    return resp\n",
    " \n",
    "# 创建文件夹的函数，保存到E盘\n",
    "def mkdir(path):    \n",
    "    isExists = os.path.exists(os.path.join(\"E:\\\\1024\", path))# os.path.exists(name)判断是否存在路径\n",
    "    # 如果不存在\n",
    "    if not isExists:\n",
    "        #print('makedir', path)\n",
    "        # 创建文件夹\n",
    "        os.makedirs(os.path.join(\"E:\\\\1024\", path))# os.path.join(path, name)连接目录与文件名\n",
    "        # 切换到创建的文件夹\n",
    "        os.chdir(os.path.join(\"E:\\\\1024\", path))\n",
    "        return True\n",
    "    # 如果存在了就返回False\n",
    "    else:\n",
    "        print(path, 'already exists')\n",
    "        return False\n",
    "    \n",
    "# 获得所有页面\n",
    "def all_page():\n",
    "    base_url ='http://p3.csgfnmdb.com/pw/thread.php?fid=21'\n",
    "    get_html(base_url).encoding='utf-8'\n",
    "    soup = BeautifulSoup(get_html(base_url).text, 'lxml')\n",
    "    urllist = []\n",
    "    t1 = soup.find_all('a')\n",
    "    #print(t1)\n",
    "    for t2 in t1:\n",
    "        t3 = t2.get('href') \n",
    "        if \"html_data\" in str(t3):\n",
    "            urllist.append('http://p3.csgfnmdb.com/pw/'+ str(t3))\n",
    "    urllist=list(set(urllist))\n",
    "    #print(urllist)\n",
    "    \n",
    "    return urllist\n",
    "# 保存图片函数，传入的参数是一页所有图片url集合\n",
    "\n",
    "# 获得图片地址调用download函数进行下载\n",
    "def get_imgs():\n",
    "    # 调用函数获得所有页面\n",
    "    #urllist = all_page()\n",
    "    for url in all_page()[4:10]:\n",
    "        new = get_html(url)\n",
    "        new.encoding='utf-8'\n",
    "        soup = BeautifulSoup(new.text, 'lxml')\n",
    "        path = soup.find('title').text.split('|')[0]\n",
    "        print(path)\n",
    "    \n",
    "        # 创建文件夹的函数\n",
    "        mkdir(path)\n",
    "        # 调用请求函数获得HTML源码\n",
    "        html = get_html(url).text\n",
    "        # 使用lxml解析器，也可以使用html.parser\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        #print(soup)\n",
    "        # css选择器\n",
    "        allimgs = soup.select('img')\n",
    "        #print(allimgs)        \n",
    "        # 调用download函数下载保存\n",
    "        download(allimgs)\n",
    "    # 执行完毕打出ok\n",
    "    print('ok')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download(list):\n",
    "    for img in list:\n",
    "        urls = img['src']\n",
    "        # 判断url是否完整\n",
    "        if urls[0:5] == 'http:':\n",
    "            img_url = urls\n",
    "        else:\n",
    "            img_url = 'http:' + urls\n",
    "        filename = img_url.split('/')[-1]\n",
    "        tpye = filename.split('.')[-1]\n",
    "        if  tpye =='jpg':\n",
    "            # 保存图片\n",
    "            with open(filename, 'wb') as f:\n",
    "                # 直接过滤掉保存失败的图片，不终止程序\n",
    "                try:\n",
    "                    f.write(get_html(img_url).content)\n",
    "                    #print('Sucessful image:',filename)\n",
    "                except:\n",
    "                    print('Failed:',filename)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 计时\n",
    "    t1 = time.time()\n",
    "    # 调用函数\n",
    "    get_imgs()\n",
    "    print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "# 发出请求获得HTML源码\n",
    "def get_html(url):\n",
    "    # 指定一个浏览器头\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}\n",
    "    # 代理，免费的代理只能维持一会可能就没用了，自行更换\n",
    "    proxies = {'http': '111.23.10.27:8080'}\n",
    "    try:\n",
    "        # Requests库的get请求\n",
    "        resp = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        # 如果请求被阻，就使用代理\n",
    "        resp = requests.get(url, headers=headers, proxies=proxies)\n",
    "        print('代理登陆中')\n",
    " \n",
    "    return resp\n",
    "#main > div:nth-child(10) > span.pages > span > a\n",
    "base_url = 'http://e1.wkcsncjdbd.info/pw/thread.php?fid=21'\n",
    "# BeautifulSoup解析页面得到最高页码数\n",
    "soup = BeautifulSoup(get_html(base_url).text, 'lxml')\n",
    "# 获得最高页码数\n",
    "#allpage = soup.find(' > span.pages > span > a')\n",
    "#allpage = soup.find_all('span', class_=\"f_one f1\")\n",
    "#allpage = soup.find_all('#a_ajax_')\n",
    "allpage = soup.select('a_aja')\n",
    "#a_ajax_3951582\n",
    "#a_ajax_3951581\n",
    "print(allpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup            #Beautiful Soup是一个可以从HTML或XML文件中提取结构化数据的Python库\n",
    " \n",
    "#构造头文件，模拟浏览器访问\n",
    "url=\"http://e1.wkcsncjdbd.info/pw/thread.php?fid=21\"\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "res = requests.get(url,headers=headers)\n",
    "res.encoding = 'utf-8'\n",
    "text = res.text\n",
    "\n",
    "soup = BeautifulSoup(text, 'lxml')\n",
    "print(soup)\n",
    "#page_info = request.urlopen(page).read().decode('utf-8')#打开Url,获取HttpResponse返回对象并读取其ResposneBody\n",
    " \n",
    "# 将获取到的内容转换成BeautifulSoup格式，并将html.parser作为解析器\n",
    "#soup = BeautifulSoup(page_info, 'html.parser')\n",
    "# 以格式化的形式打印html\n",
    "\n",
    "#print(soup.prettify())\n",
    " \n",
    "#titles = soup.find_all('a','03.13')# 查找所有a标签中class='title'的语句\n",
    "titles = soup.select('03.13')\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def all_page():\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}  \n",
    "    # Requests库的get请求\n",
    "    url = 'http://p3.csgfnmdb.com/pw/html_data/14/1903/3953947.html'\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    #url = 'http://p3.csgfnmdb.com/pw/thread.php?fid=14'\n",
    "    \n",
    "    # BeautifulSoup解析页面得到最高页码数\n",
    "    resp.encoding='utf-8'\n",
    "    soup = BeautifulSoup(resp.text, 'lxml')\n",
    "    #print(soup)\n",
    "    # 获得最高页码数\n",
    "    #allpage = soup.find(' > span.pages > span > a')\n",
    "    #allpage = soup.find('a_ajax_')\n",
    "    allpage = soup.select('read_tpc > img:nth-')\n",
    "    #a_ajax_3953947\n",
    "    print(str(allpage)+'lala')\n",
    "    urllist = []\n",
    "    # for循环迭代出所有页面，得到url\n",
    "    #for page in range(1, int(allpage)+1):\n",
    "    for page in range(1, 2):\n",
    "        allurl = base_url + str(page+3953938)+'.html'\n",
    "        urllist.append(allurl)\n",
    "    print(urllist)\n",
    "    \n",
    "    return urllist\n",
    "\n",
    "all_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取所有的链接\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}  \n",
    "# Requests库的get请求\n",
    "url = 'http://p3.csgfnmdb.com/pw/html_data/21/1903/3956893.html'\n",
    "resp = requests.get(url, headers=headers)\n",
    "#url = 'http://p3.csgfnmdb.com/pw/thread.php?fid=14'\n",
    "resp.encoding='utf-8'\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "t1 = soup.find_all(re.compile('title'))\n",
    "print(t1)\n",
    "# urllist = []\n",
    "# for t2 in t1:\n",
    "#     t3 = t2.get('href') \n",
    "#     if \"html_data\" in str(t3):\n",
    "#         urllist.append('http://p3.csgfnmdb.com/pw/'+ str(t3))\n",
    "# print(list(set(urllist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
